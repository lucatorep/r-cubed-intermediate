{
  "hash": "4f76483ee42cb55f63e39fd0da1b5b54",
  "result": {
    "markdown": "# Processing and joining datasets for cleaning {#sec-dplyr-joins}\n\n\n\n\n\nHere we will continue using the \"*Workflow*\" block and start moving over\nto the third block, \"*Create project data*\", in\n@fig-overview-create-project-data.\n\n![Section of the overall workflow we will be\ncovering.](/images/overview-create-project-data.svg){#fig-overview-create-project-data}\n\n## Learning objectives\n\n1.  Learn what regular expressions are and how to use them on character\n    data.\n2.  Learn about and apply the various ways data can be joined.\n3.  Apply functionals when repeatedly joining multiple datasets.\n4.  Apply the function `case_when()` when you need nested conditionals\n    for cleaning.\n5.  Use the `usethis::use_data()` function to save the final, fully\n    joined dataset as an `.Rda` file in `data/`.\n\n## Processing character data\n\n::: callout-note\n## Reading task: \\~5 minutes\n\nBefore we go into joining datasets together, we have to do a bit of\nprocessing first. Specifically, we want to get the user ID from the\n`file_path_id` character data. Whenever you are processing and cleaning\ndata, you will very likely encounter and deal with character data. A\nwonderful package to use for working with character data is called\n`{stringr}`, which we'll use to extract the user ID from the\n`file_path_id` column.\n\nThe main driver behind the functions in stringr are [regular\nexpressions](https://en.wikipedia.org/wiki/Regular_expression) (or regex\nfor short). These expressions are powerful, very concise ways of finding\npatterns in text. Because they are so concise, though, they are also\n*very very difficult* to learn, write, and read, even for experienced\nusers. That's because certain characters like `[` or `?` have special\nmeanings. For instance, `[aeiou]` is regex for \"find one character in a\nstring that is either a, e, i, o, or u\". The `[]` in this case mean\n\"find the character in between the two brackets\". We won't cover regex\ntoo much in this course, some great resources for learning them are the\n[R for Data Science regex\nsection](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions),\nthe [stringr regex\npage](https://stringr.tidyverse.org/articles/regular-expressions.html),\nas well as in the help doc `?regex`.\n\nWe've already used them a bit in the `dir_ls()` function with the\n`regexp` argument to find our data files. In the case of the regex in\nour use of `dir_ls()`, we had wanted to find, for instance, the pattern\n`\"user_info.csv\"` in all the folders and files. But in this case, we\nwant to extract the user ID pattern, from `user_1` to `user_22`. So how\nwould we go about extracting this pattern?\n:::\n\n## Exercise: Brainstorm a regex that will match for the user ID\n\n> Time: 10 minutes.\n\nIn your groups do these tasks. Try not to look ahead, nor in the\nsolution section :wink:! When the time is up, we'll share some ideas and\ngo over what the regex will be.\n\n1.  Looking at the `file_path_id` column, list what is similar in the\n    user ID between rows and what is different.\n2.  Discuss and verbally describe (in English, not regex) what text\n    pattern you might use to extract the user ID.\n3.  Use the list below to think about how you might convert the English\n    description of the text pattern to a regex. This will probably be\n    very hard, but try anyway.\n    -   When characters are written as is, regex will find those\n        characters, e.g. `user` will find only `user`.\n    -   Use `[]` to find one possible character of the several between\n        the brackets. E.g. `[12]` means 1 *or* 2 or `[ab]` means \"a\" or\n        \"b\". To find a range of numbers or letters, use `-` in between\n        the start and end ranges, e.g. `[1-3]` means 1 to 3 or `[a-c]`\n        means \"a\" to \"c\".\n    -   Use `?` if the character might be there or not. E.g. `ab?` means\n        \"a\" and maybe \"b\" follows it or `1[1-2]?` means 1 and maybe 1 or\n        2 will follow it.\n\nOnce you've done these tasks, we'll discuss all together and go over\nwhat the regex would be to extract the user ID.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for the solution**. Only click if you are struggling or are out of time.\"}\n\"user_[1-9][0-9]?\"\n```\n:::\n\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nMake sure to reinforce that while regex is incredibly complicated, there\nare some basic things you can do with it that are quite powerful.\n\nMore or less, this section and exercise are to introduce the idea and\nconcept of regex, but not to really teach it since that is well beyond\nthe scope of this course and this time frame.\n\nGo over the solution. Explanation is that the pattern will find anything\nthat has `user_` followed by a number from 1 to 9 and maybe followed by\nanother number from 0 to 9.\n:::\n\n## Using regular expressions to extract text\n\nNow that we've identified a possible regex to use to extract the user\nID, let's test it out on the `user_info_df` data. Once it works, we will\nconvert it into a function and move it into the `R/functions.R` file.\n\nSince we will create a new column for the user ID, we will use the\n`mutate()` function from the `{dplyr}` package. We'll use the\n`str_extract()` function from the `{stringr}` package to \"extract a\nstring\" by using the regex `user_[1-9][0-9]?` that we discussed from the\nexercise. We're also using an argument to `mutate()` you might not have\nseen previously, called `.before`. This will insert the new `user_id`\ncolumn before the column we use and we do this entirely for visual\nreasons, since it is easier to see the newly created column when we run\nthe code. In your `doc/learning.qmd` file, create a new header called\n`## Using regex for user ID` at the bottom of the document, and create a\nnew code chunk below that.\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nWalk through writing this code, briefly explain/remind how to use\nmutate, and about the stringr function.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_info_df <- import_multiple_files(\"user_info.csv\", import_user_info)\n# Note: your file paths and data may look slightly different.\nuser_info_df %>%\n  mutate(\n    user_id = str_extract(\n      file_path_id,\n      \"user_[1-9][0-9]?\"\n    ),\n    .before = file_path_id\n  )\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 22 × 6\n#>    user_id file_path_id                   gender weight height   age\n#>    <chr>   <chr>                          <chr>   <dbl>  <dbl> <dbl>\n#>  1 user_1  data-raw/mmash/user_1/user_in… M          65    169    29\n#>  2 user_10 data-raw/mmash/user_10/user_i… M          85    180    27\n#>  3 user_11 data-raw/mmash/user_11/user_i… M         115    186    27\n#>  4 user_12 data-raw/mmash/user_12/user_i… M          67    170    27\n#>  5 user_13 data-raw/mmash/user_13/user_i… M          74    180    25\n#>  6 user_14 data-raw/mmash/user_14/user_i… M          64    171    27\n#>  7 user_15 data-raw/mmash/user_15/user_i… M          80    180    24\n#>  8 user_16 data-raw/mmash/user_16/user_i… M          67    176    27\n#>  9 user_17 data-raw/mmash/user_17/user_i… M          60    175    24\n#> 10 user_18 data-raw/mmash/user_18/user_i… M          80    180     0\n#> # ℹ 12 more rows\n```\n:::\n:::\n\n\nSince we don't need the `file_path_id` column anymore, let's drop it\nusing `select()` and `-`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_info_df %>%\n  mutate(\n    user_id = str_extract(\n      file_path_id,\n      \"user_[1-9][0-9]?\"\n    ),\n    .before = file_path_id\n  ) %>%\n  select(-file_path_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 22 × 5\n#>    user_id gender weight height   age\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl>\n#>  1 user_1  M          65    169    29\n#>  2 user_10 M          85    180    27\n#>  3 user_11 M         115    186    27\n#>  4 user_12 M          67    170    27\n#>  5 user_13 M          74    180    25\n#>  6 user_14 M          64    171    27\n#>  7 user_15 M          80    180    24\n#>  8 user_16 M          67    176    27\n#>  9 user_17 M          60    175    24\n#> 10 user_18 M          80    180     0\n#> # ℹ 12 more rows\n```\n:::\n:::\n\n\n## Exercise: Convert ID extractor code into a function\n\n> Time: 15 minutes.\n\nWe now have code that takes the data that has the `file_path_id` column\nand extracts the user ID from it. **First step**: While in the\n`doc/learning.qmd` file, convert this code into a function, using the\nsame process you've done previously.\n\n1.  Call the new function `extract_user_id` and add one argument called\n    `imported_data`. - Remember to output the code into an object and\n    `return()` it at the end of the function. - Include Roxygen\n    documentation.\n2.  After writing it and testing that the function works, move the\n    function into `R/functions.R`.\n3.  Run `{styler}` while in the `R/functions.R` file with\n    {{< var keybind.styler >}}.\n4.  Replace the code in the `doc/learning.qmd` file with the function\n    name so it looks like `extract_user_id(user_info_df)`, restart the R\n    session, source everything with `source()` with\n    {{< var keybind.source >}}, and run the new function to test that it\n    works.\n5.  Knit / render the `doc/learning.qmd` file to make sure things remain\n    reproducible with {{< var keybind.render >}}.\n6.  Add and commit the changes to the Git history with\n    {{< var keybind.git >}}.\n\n::: callout-tip\nIf you don't know what package a function comes from when you need to\nappend the package when using `::`, you can find out what the package is\nby using the help documentation `?functionname` (can also be done by\npressing F1 when the cursor is over the function). The package name is\nat the very top left corner, surrounded by `{ }`.\n:::\n\nUse this code as a guide to help complete this exercise:\n\n``` r\nextract_user_id <- ___(___) {\n    ___ <- ___ %>%\n        ___mutate(\n            user_id = ___str_extract(file_path_id,\n                                     \"user_[0-9][0-9]?\"),\n            .before = file_path_id\n        ) %>%\n        ___select(-file_path_id)\n    return(___)\n}\n\n# This tests that it works:\n# extract_user_id(user_info_df)\n```\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for the solution**. Only click if you are struggling or are out of time.\"}\n#' Extract user ID from data with file path column.\n#'\n#' @param imported_data Data with `file_path_id` column.\n#'\n#' @return A data.frame/tibble.\n#'\nextract_user_id <- function(imported_data) {\n  extracted_id <- imported_data %>%\n    dplyr::mutate(\n      user_id = stringr::str_extract(\n        file_path_id,\n        \"user_[0-9][0-9]?\"\n      ),\n      .before = file_path_id\n    ) %>%\n    dplyr::select(-file_path_id)\n  return(extracted_id)\n}\n\n# This tests that it works:\nextract_user_id(user_info_df)\n```\n:::\n\n\n## Modifying existing functions as part of the processing workflow\n\nNow that we've created a new function to extract the user ID from the\nfile path variable, we need to actually use it within our processing\npipeline. Since we want this function to work on all the datasets that\nwe will import, we need to add it to the `import_multiple_files()`\nfunction. We'll go to the `import_multiple_files()` function in\n`R/functions.R` and use the `%>%` to add it after using the `map_dfr()`\nfunction. The code should look something like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimport_multiple_files <- function(file_pattern, import_function) {\n  data_files <- fs::dir_ls(here::here(\"data-raw/mmash/\"),\n    regexp = file_pattern,\n    recurse = TRUE\n  )\n\n  combined_data <- purrr::map_dfr(data_files, import_function,\n    .id = \"file_path_id\"\n  ) %>%\n    extract_user_id() # Add the function here.\n  return(combined_data)\n}\n```\n:::\n\n\nWe'll re-source the functions with `source()` using\n{{< var keybind.source >}}. Then re-run these pieces of code you wrote\nduring the exercise in @sec-ex-function-import-all-data to update them\nbased on the new code in the `import_multiple_files()` function. Add\nthis to your `doc/learning.qmd` file for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_info_df <- import_multiple_files(\"user_info.csv\", import_user_info)\nsaliva_df <- import_multiple_files(\"saliva.csv\", import_saliva)\nrr_df <- import_multiple_files(\"RR.csv\", import_rr)\nactigraph_df <- import_multiple_files(\"Actigraph.csv\", import_actigraph)\n```\n:::\n\n\nAs well as adding the `summarised_rr_df` and `summarised_actigraph_df`\nto use `user_id` instead of `file_path_id`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarised_rr_df <- rr_df %>%\n  group_by(user_id, day) %>%\n  summarise(across(ibi_s, list(\n    mean = ~ mean(.x, na.rm = TRUE), \n    sd = ~ sd(.x, na.rm = TRUE)\n  ))) %>% \n  ungroup()\n\nsummarised_actigraph_df <- actigraph_df %>%\n  group_by(user_id, day) %>%\n  # These statistics will probably be different for you\n  summarise(across(hr, list(\n    mean = ~ mean(.x, na.rm = TRUE), \n    sd = ~ sd(.x, na.rm = TRUE)\n  ))) %>% \n  ungroup()\n```\n:::\n\n\nLet's render the `doc/learning.qmd` document using\n{{< var keybind.render >}}to make sure everything still runs fine. Then,\n**add and commit** all the changed files into the Git history with\n{{< var keybind.git >}}.\n\n## Join datasets together\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nWalk through and describe these images and the different type of joins\nafter they've read it.\n:::\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nThe ability to join datasets together is a fundamental component of data\nprocessing and transformation. In our case, we want to add the datasets\ntogether so we eventually have preferably one main dataset to work with.\n\nThere are many ways to join datasets in `{dplyr}` that are described in\n`?dplyr::join`. The more common ones that are implemented in the\n`{dplyr}` package are:\n\n-   `left_join(x, y)`: Join all rows and columns in `y` that match rows\n    and columns in `x`. *Columns* that exist in `y` but not `x` are\n    joined to `x`.\n\n![Left joining in `{dplyr}`. Notice how the row with `D` in column `A`\nin the blue data is not included in the outputted data on the right.\nModified from the RStudio `{dplyr}`\n[cheatsheet](https://posit.co/wp-content/uploads/2022/10/data-transformation-1.pdf).](/images/left-join.png){#fig-left-join\nwidth=\"90%\"}\n\n-   `right_join(x, y)`: The opposite of `left_join()`. Join all rows and\n    columns in `x` that match rows and columns in `y`. *Columns* that\n    exist in `x` but not `y` are joined to `y`.\n\n![Right joining in `{dplyr}`. Notice how the row with `C` in column `A`\nin the green data is not included in the outputted data on the right.\nModified from the RStudio `{dplyr}`\n[cheatsheet](https://posit.co/wp-content/uploads/2022/10/data-transformation-1.pdf).](/images/right-join.png){#fig-right-join\nwidth=\"90%\"}\n\n-   `full_join(x, y)`: Join all rows and columns in `y` that match rows\n    and columns in `x`. Columns *and* **rows** that exist in `y` but not\n    `x` are joined to `x`.\n\n![Full joining in `{dplyr}`. Notice how all rows and columns are\nincluded in the outputted data on the right, and that some missingness\nis introduced because those values don't exist when the data are\ncombined in this way. Modified from the RStudio `{dplyr}`\n[cheatsheet](https://posit.co/wp-content/uploads/2022/10/data-transformation-1.pdf).](/images/full-join.png){#fig-full-join\nwidth=\"90%\"}\n\nIn our case, we want to use `full_join()`, since we want all the data\nfrom both datasets. This function takes two datasets and lets you\nindicate which column to join by using the `by` argument. Here, both\ndatasets have the column `user_id` so we will join by them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_join(user_info_df, saliva_df, by = \"user_id\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 43 × 8\n#>    user_id gender weight height   age samples      cortisol_norm\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl> <chr>                <dbl>\n#>  1 user_1  M          65    169    29 before sleep        0.0341\n#>  2 user_1  M          65    169    29 wake up             0.0779\n#>  3 user_10 M          85    180    27 before sleep        0.0370\n#>  4 user_10 M          85    180    27 wake up             0.0197\n#>  5 user_11 M         115    186    27 before sleep        0.0406\n#>  6 user_11 M         115    186    27 wake up             0.0156\n#>  7 user_12 M          67    170    27 before sleep        0.156 \n#>  8 user_12 M          67    170    27 wake up             0.145 \n#>  9 user_13 M          74    180    25 before sleep        0.0123\n#> 10 user_13 M          74    180    25 wake up             0.0342\n#> # ℹ 33 more rows\n#> # ℹ 1 more variable: melatonin_norm <dbl>\n```\n:::\n:::\n\n\n`full_join()` is useful if we want to include all values from both\ndatasets, as long as each participant (\"user\") had data collected from\nthat dataset. When the two datasets have rows that don't match, we will\nget missingness in that row, but that's ok in this case.\n\nWe also eventually have other datasets to join together later on. Since\n`full_join()` can only take two datasets at a time, do we then just keep\nusing `full_join()` until all the other datasets are combined? What if\nwe get more data later on? Well, that's where more functional\nprogramming comes in. Again, we have a simple goal: For a set of data\nframes, join them all together. Here we use another functional\nprogramming concept called `reduce()`. Like `map()`, which \"maps\" a\nfunction onto a set of items, `reduce()` applies a function to each item\nof a vector or list, each time reducing the set of items down until only\none remains: the output. Let's use an example with our simple function\n`add_numbers()` we had created before (but later deleted) and add up 1\nto 5. Since `add_numbers()` only takes two numbers, we have to give it\ntwo numbers at a time and repeat until we reach 5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add from 1 to 5\nfirst <- add_numbers(1, 2)\nsecond <- add_numbers(first, 3)\nthird <- add_numbers(second, 4)\nadd_numbers(third, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> [1] 15\n```\n:::\n:::\n\n\nInstead, we can use reduce to do the same thing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce(1:5, add_numbers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> [1] 15\n```\n:::\n:::\n\n\n@fig-reduce visually shows what is happening within `reduce()`.\n\n![A functional that iteratively uses a function on a set of items until\nonly one output remains. Notice how the output of the first iteration of\nthe `func()` function is placed in the first position of `func()` in the\nnext iteration, and so on. Modified from the RStudio purrr\n[cheatsheet](https://posit.co/wp-content/uploads/2022/10/purrr.pdf).](/images/reduce.png){#fig-reduce\nwidth=\"90%\"}\n\nIf we look at `?reduce`, we see that `reduce()`, like `map()`, takes\neither a vector or a list as an input. Since data frames can only be put\ntogether as a list and not as a vector (a data frame has vectors for\ncolumns and so can't be a vector itself), we need to combine the\ndatasets together in a `list()` and reduce them with `full_join()`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombined_data <- reduce(list(user_info_df, saliva_df), full_join)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Joining with `by = join_by(user_id)`\n```\n:::\n\n```{.r .cell-code}\ncombined_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 43 × 8\n#>    user_id gender weight height   age samples      cortisol_norm\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl> <chr>                <dbl>\n#>  1 user_1  M          65    169    29 before sleep        0.0341\n#>  2 user_1  M          65    169    29 wake up             0.0779\n#>  3 user_10 M          85    180    27 before sleep        0.0370\n#>  4 user_10 M          85    180    27 wake up             0.0197\n#>  5 user_11 M         115    186    27 before sleep        0.0406\n#>  6 user_11 M         115    186    27 wake up             0.0156\n#>  7 user_12 M          67    170    27 before sleep        0.156 \n#>  8 user_12 M          67    170    27 wake up             0.145 \n#>  9 user_13 M          74    180    25 before sleep        0.0123\n#> 10 user_13 M          74    180    25 wake up             0.0342\n#> # ℹ 33 more rows\n#> # ℹ 1 more variable: melatonin_norm <dbl>\n```\n:::\n:::\n\n\nWe now have the data in a form that would make sense to join it with the\nother datasets. So lets try it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce(list(user_info_df, saliva_df, summarised_rr_df), full_join)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Joining with `by = join_by(user_id)`\n#> Joining with `by = join_by(user_id)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 86 × 11\n#>    user_id gender weight height   age samples      cortisol_norm\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl> <chr>                <dbl>\n#>  1 user_1  M          65    169    29 before sleep        0.0341\n#>  2 user_1  M          65    169    29 before sleep        0.0341\n#>  3 user_1  M          65    169    29 wake up             0.0779\n#>  4 user_1  M          65    169    29 wake up             0.0779\n#>  5 user_10 M          85    180    27 before sleep        0.0370\n#>  6 user_10 M          85    180    27 before sleep        0.0370\n#>  7 user_10 M          85    180    27 wake up             0.0197\n#>  8 user_10 M          85    180    27 wake up             0.0197\n#>  9 user_11 M         115    186    27 before sleep        0.0406\n#> 10 user_11 M         115    186    27 before sleep        0.0406\n#> # ℹ 76 more rows\n#> # ℹ 4 more variables: melatonin_norm <dbl>, day <dbl>,\n#> #   ibi_s_mean <dbl>, ibi_s_sd <dbl>\n```\n:::\n:::\n\n\nHmm, but wait, we now have four rows of each user, when we should have\nonly two, one for each day. By looking at each dataset we joined, we can\nfind that the `saliva_df` doesn't have a `day` column and instead has a\n`samples` column. We'll need to add a day column in order to join\nproperly with the RR dataset. For this, we'll learn about using nested\nconditionals.\n\n## Cleaning with nested conditionals\n\n::: callout-note\n## Reading task: \\~6 minutes\n\nThere are many ways to clean up this particular problem, but probably\nthe easiest, most explicit, and programmatically accurate way of doing\nit would be with the function `case_when()`. This function works by\nproviding it with a series of logical conditions and an associated\noutput if the condition is true. Each condition is processed\n*sequentially*, meaning that if a condition is TRUE, the output won't be\noverridden for later conditions. The general form of `case_when()` looks\nlike:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_when(\n  variable1 == condition1 ~ output,\n  variable2 == condition2 ~ output,\n  # (Optional) Otherwise\n  TRUE ~ final_output\n)\n```\n:::\n\n\nThe optional ending is only necessary if you want a certain output if\nnone of your conditions are met. Because conditions are processed\n*sequentially* and because it is the *last* condition, by setting it as\n`TRUE` the final output will used. If this last `TRUE` condition is not\nused then by default, the final output would be a missing value. A\n(silly) example using age might be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_when(\n  age > 20 ~ \"old\",\n  age <= 20 ~ \"young\",\n  # For final condition\n  TRUE ~ \"unborn!\"\n)\n```\n:::\n\n\nIf instead you want one of the conditions to be `NA`, you need to set\nthe appropriate `NA` value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_when(\n  age > 20 ~ \"old\",\n  age <= 20 ~ NA_character_,\n  # For final condition\n  TRUE ~ \"unborn!\"\n)\n```\n:::\n\n\nAlternatively, if we want missing age values to output `NA` values at\nthe end (instead of `\"unborn!\"`), we would exclude the final condition:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncase_when(\n  age > 20 ~ \"old\",\n  age <= 20 ~ \"young\"\n)\n```\n:::\n\n\nWith `{dplyr}` functions like `case_when()`, it requires you be explicit\nabout the type of output each condition has since all the outputs must\nmatch (e.g. all character or all numeric). This prevents you from\naccidentally mixing e.g. numeric output with character output. Missing\nvalues also have data types:\n\n-   `NA_character_` (character)\n-   `NA_real_` (numeric)\n-   `NA_integer_` (integer)\n\nAssuming the final output is `NA`, in a pipeline this would look like\nhow you normally would use `mutate()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuser_info_df %>%\n  mutate(age_category = case_when(\n    age > 20 ~ \"old\",\n    age <= 20 ~ \"young\"\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 22 × 6\n#>    user_id gender weight height   age age_category\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl> <chr>       \n#>  1 user_1  M          65    169    29 old         \n#>  2 user_10 M          85    180    27 old         \n#>  3 user_11 M         115    186    27 old         \n#>  4 user_12 M          67    170    27 old         \n#>  5 user_13 M          74    180    25 old         \n#>  6 user_14 M          64    171    27 old         \n#>  7 user_15 M          80    180    24 old         \n#>  8 user_16 M          67    176    27 old         \n#>  9 user_17 M          60    175    24 old         \n#> 10 user_18 M          80    180     0 young       \n#> # ℹ 12 more rows\n```\n:::\n:::\n\n:::\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nBriefly review the content again, to reinforce what they read.\n:::\n\nBy using the `case_when()` function, we can set `\"before sleep\"` as day\n1 and `\"wake up\"` as day 2 by creating a new column called `day`. (We\nwill use `NA_real_` because the other `day` columns are numeric, not\ninteger.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaliva_with_day_df <- saliva_df %>%\n  mutate(day = case_when(\n    samples == \"before sleep\" ~ 1,\n    samples == \"wake up\" ~ 2\n  ))\nsaliva_with_day_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 42 × 5\n#>    user_id samples      cortisol_norm melatonin_norm   day\n#>    <chr>   <chr>                <dbl>          <dbl> <dbl>\n#>  1 user_1  before sleep        0.0341  0.0000000174      1\n#>  2 user_1  wake up             0.0779  0.00000000675     2\n#>  3 user_10 before sleep        0.0370  0.00000000867     1\n#>  4 user_10 wake up             0.0197  0.00000000257     2\n#>  5 user_11 before sleep        0.0406  0.00000000204     1\n#>  6 user_11 wake up             0.0156  0.00000000965     2\n#>  7 user_12 before sleep        0.156   0.00000000354     1\n#>  8 user_12 wake up             0.145   0.00000000864     2\n#>  9 user_13 before sleep        0.0123  0.00000000190     1\n#> 10 user_13 wake up             0.0342  0.00000000230     2\n#> # ℹ 32 more rows\n```\n:::\n:::\n\n\n...Now, let's use the `reduce()` with `full_join()` again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce(list(user_info_df, saliva_with_day_df, summarised_rr_df), full_join)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Joining with `by = join_by(user_id)`\n#> Joining with `by = join_by(user_id, day)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n#> # A tibble: 47 × 11\n#>    user_id gender weight height   age samples      cortisol_norm\n#>    <chr>   <chr>   <dbl>  <dbl> <dbl> <chr>                <dbl>\n#>  1 user_1  M          65    169    29 before sleep        0.0341\n#>  2 user_1  M          65    169    29 wake up             0.0779\n#>  3 user_10 M          85    180    27 before sleep        0.0370\n#>  4 user_10 M          85    180    27 wake up             0.0197\n#>  5 user_11 M         115    186    27 before sleep        0.0406\n#>  6 user_11 M         115    186    27 wake up             0.0156\n#>  7 user_12 M          67    170    27 before sleep        0.156 \n#>  8 user_12 M          67    170    27 wake up             0.145 \n#>  9 user_13 M          74    180    25 before sleep        0.0123\n#> 10 user_13 M          74    180    25 wake up             0.0342\n#> # ℹ 37 more rows\n#> # ℹ 4 more variables: melatonin_norm <dbl>, day <dbl>,\n#> #   ibi_s_mean <dbl>, ibi_s_sd <dbl>\n```\n:::\n:::\n\n\nWe now have two rows per participant! Let's add and commit the changes\nto the Git history with {{< var keybind.git >}}.\n\n## Wrangling data into final form\n\nNow that we've got several datasets processed and joined, its now time\nto bring it all together and put it into the `data-raw/mmash.R` script\nso we can create a final working dataset.\n\nOpen up the `data-raw/mmash.R` file and the top of the file, add the\n`{vroom}` package to the end of the list of other packages. Move the\ncode `library(fs)` to go with the other packages as well. It should look\nsomething like this now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(fs)\n```\n:::\n\n\nThen, we will comment out the `download.file()`, `unzip()`,\n`file_delete()`, and `file_move()` code, since we don't want to download\nand unzip the data every single time we run this script. It should look\nlike this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download\nmmash_link <- \"https://physionet.org/static/published-projects/mmash/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0.zip\"\n# download.file(mmash_link, destfile = here(\"data-raw/mmash-data.zip\"))\n\n# Unzip\n# unzip(here(\"data-raw/mmash-data.zip\"),\n#       exdir = here(\"data-raw\"),\n#       junkpaths = TRUE)\n# unzip(here(\"data-raw/MMASH.zip\"),\n#       exdir = here(\"data-raw\"))\n\n# Remove/tidy up left over files\n# file_delete(here(c(\"data-raw/MMASH.zip\",\n#                    \"data-raw/SHA256SUMS.txt\",\n#                    \"data-raw/LICENSE.txt\")))\n# file_move(here(\"data-raw/DataPaper\"), here(\"data-raw/mmash\"))\n```\n:::\n\n\nGo into the `doc/learning.qmd` and cut the code used to create the\n`saliva_with_day_df` as well as the code to `full_join()` all the\ndatasets together with `reduce()` and paste it at the bottom of the\n`data-raw/mmash.R` script. Assign the output into a new variable called\n`mmash`, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaliva_with_day_df <- saliva_df %>%\n  mutate(day = case_when(\n    samples == \"before sleep\" ~ 1,\n    samples == \"wake up\" ~ 2,\n    TRUE ~ NA_real_\n  ))\n\nmmash <- reduce(\n  list(\n    user_info_df,\n    saliva_with_day_df,\n    summarised_rr_df,\n    summarised_actigraph_df\n  ),\n  full_join\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Joining with `by = join_by(user_id)`\n#> Joining with `by = join_by(user_id, day)`\n#> Joining with `by = join_by(user_id, day)`\n```\n:::\n:::\n\n\nLastly, we have to save this final dataset into the `data/` folder.\nWe'll use the function `usethis::use_data()` to create the folder and\nsave the data as an `.rda` file. We'll add this code to the very bottom\nof the script:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusethis::use_data(mmash, overwrite = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> ✔ Setting active project to '/home/luke/Documents/rostools/r-cubed-intermediate'\n#> ✔ Saving 'mmash' to 'data/mmash.rda'\n#> • Document your data (see 'https://r-pkgs.org/data.html')\n```\n:::\n:::\n\n\nWe're adding `overwrite = TRUE` so every time we re-run this script, the\ndataset will be saved. If the final dataset is going to be really large,\nwe could (but won't in this course) save it as a `.csv` file with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvroom_write(mmash, here(\"data/mmash.csv\"))\n```\n:::\n\n\nAnd later load it in with `vroom()` (since it is so fast). Alright,\nwe're finished creating this dataset! Let's generate it by:\n\n-   First running `{styler}` with {{< var keybind.styler >}}.\n-   Restarting the R session with {{< var keybind.restart-r >}}.\n-   Sourcing the `data-raw/mmash.R` script with\n    {{< var keybind.source >}}.\n\nWe now have a final dataset to start working on! The main way to load\ndata is with `load(here::here(\"data/mmash.rda\"))`. Go into the\n`doc/learning.qmd` file and delete **everything** again, except for the\nYAML header and `setup` code chunk, so that we are ready for the next\nsession. Lastly, **add and commit** all the changes, including adding\nthe final `mmash.rda` data file, to the Git history by using\n{{< var keybind.git >}}.\n\n## Summary\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nQuickly cover this before finishing the session and when starting the\nnext session.\n:::\n\n-   While very difficult to learn and use, regular expressions (regex or\n    regexp) are incredibly powerfully at processing character data.\n-   Use `left_join()`, `right_join()`, and `full_join()` to join two\n    datasets together.\n-   Use the functional `reduce()` to iteratively apply a function to a\n    set of items in order to end up with one item (e.g. join more than\n    two datasets into one final dataset).\n-   Use `case_when()` instead of nesting multiple \"if else\" conditions\n    whenever you need to do slightly more complicated conditionals.\n\n\n\n",
    "supporting": [
      "dplyr-joins_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}